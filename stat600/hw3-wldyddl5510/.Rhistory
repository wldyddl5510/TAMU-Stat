(sum(max.col(package_model$fitted.values) != iris_yt) / length(iris_yt))
max.col(package_model$fitted.values)
(sum(max.col(package_model$fitted.values) != iris_y) / length(iris_yt))
(sum(max.col(package_model$fitted.values) != iris_y) / length(iris_y))
(sum(max.col(package_model$fitted.values) != iris_y) / length(iris_y)) * 100
sum(max.col(package_model$fitted.values) != iris_y)
(sum(max.col(package_model$fitted.values) == iris_y) / length(iris_y)) * 100
fitted(package_model)
max.col(fitted(package_model))
iris_y
(sum(max.col(package_model$fitted.values) != (iris_y + 1)) / length(iris_y)) * 100
# error of mine
my_model
# normal cases
source("FunctionsLR.R")
# actual dataset: iris
iris_data <- iris
# convert y to 0~k class
iris_data$Species <- as.numeric(iris_data$Species) - 1
# permute
iris_data = iris_data[sample(nrow(iris_data)), ]
iris_data
# divide train and test
iris_train = iris_data[1:((0.8) * nrow(iris_data)), ]
iris_train
iris_test = iris_data[(0.8 * nrow(iris_data)):nrow(iris_data), ]
iris_test
# y
iris_y = as.vector(iris_train$Species)
iris_yt = as.vector(iris_test$Species)
# X
iris_X = cbind(rep(1, nrow(iris_train)), iris_train[, -ncol(iris_train)])
iris_X = as.matrix(iris_X)
iris_Xt = cbind(rep(1, nrow(iris_test)), iris_test[, -ncol(iris_test)])
iris_Xt = as.matrix(iris_Xt)
# compare performance
library(nnet)
iris_package = cbind(rep(1, nrow(iris_train)), iris_train)
iris_package
package_model <- multinom(Species ~ ., data = iris_package)
my_model <- LRMultiClass(iris_X, iris_y, iris_Xt, iris_yt, numIter = 100)
# error by package
(sum(max.col(package_model$fitted.values) != (iris_y + 1)) / length(iris_y)) * 100
# error of mine
my_model
microbenchmark(
LRMultiClass(iris_X, iris_y, iris_Xt, iris_yt, numIter = 100),
multinom(Species ~ ., data = iris_package)
)
# Application of multi-class logistic to letters data
# Load the letter data
#########################
# Training data
letter_train <- read.table("Data/letter-train.txt", header = F, colClasses = "numeric")
Y <- letter_train[, 1]
X <- as.matrix(letter_train[, -1])
# Testing data
letter_test <- read.table("Data/letter-test.txt", header = F, colClasses = "numeric")
Yt <- letter_test[, 1]
Xt <- as.matrix(letter_test[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n = nrow(X)
ntest = nrow(Xt)
X = cbind(rep(1, n), X)
Xt = cbind(rep(1, ntest), Xt)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X, Y, Xt, Yt)
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out$error_train, type = 'o')
plot(out$error_test, type = 'o')
# result of the last step
print(out$objective[length(out$objective)])
print(out$error_train[length(out$error_train)])
print(out$error_test[length(out$error_test)])
# Feel free to modify the code above for different lambda/eta/numIter values to see how it affects the convergence as well as train/test errors
# larger and smaller lambda
out_lambda2 = LRMultiClass(X, Y, Xt, Yt, lambda = 2)
out_lambda05 = LRMultiClass(X, Y, Xt, Yt, lambda = 0.5)
# objective with diff lambda
plot(out_lambda2$objective, type = 'o')
plot(out_lambda05$objective, type = 'o')
# compare last step
print(out_lambda2$objective[length(out_lambda2$objective)])
print(out_lambda05$objective[length(out_lambda05$objective)])
# train error with diff lambda
plot(out_lambda2$error_train, type = 'o')
plot(out_lambda05$error_train, type = 'o')
print(out_lambda2$error_train[length(out_lambda2$error_train)])
print(out_lambda05$error_train[length(out_lambda05$error_train)])
# test error with diff lambda
plot(out_lambda2$error_test, type = 'o')
plot(out_lambda05$error_test, type = 'o')
print(out_lambda2$error_test[length(out_lambda2$error_test)])
print(out_lambda05$error_test[length(out_lambda05$error_test)])
# larger and smaller eta
out_eta05 = LRMultiClass(X, Y, Xt, Yt, eta = 0.5)
out_eta001 = LRMultiClass(X, Y, Xt, Yt, eta = 0.01)
# objective with diff eta
plot(out_eta05$objective, type = 'o')
plot(out_eta001$objective, type = 'o')
print(out_eta05$objective[length(out_eta05$objective)])
print(out_eta001$objective[length(out_eta001$objective)])
# train error with diff eta
plot(out_eta05$error_train, type = 'o')
plot(out_eta001$error_train, type = 'o')
print(out_eta05$error_train[length(out_eta05$error_train)])
print(out_eta001$error_train[length(out_eta001$error_train)])
# test error with diff eta
plot(out_eta05$error_test, type = 'o')
plot(out_eta001$error_test, type = 'o')
print(out_eta05$error_test[length(out_eta05$error_test)])
print(out_eta001$error_test[length(out_eta001$error_test)])
# larger and smaller Numiter
out_iter25 = LRMultiClass(X, Y, Xt, Yt, numIter = 25)
out_iter100 = LRMultiClass(X, Y, Xt, Yt, numIter = 100)
# objective with diff numIter
plot(out_iter25$objective, type = 'o')
plot(out_iter100$objective, type = 'o')
print(out_iter25$objective[length(out_iter25$objective)])
print(out_iter100$objective[length(out_iter100$objective)])
# train error with diff iter
plot(out_iter25$error_train, type = 'o')
plot(out_iter100$error_train, type = 'o')
print(out_iter25$error_train[length(out_iter25$error_train)])
print(out_iter100$error_train[length(out_iter100$error_train)])
# test error with diff iter
plot(out_iter25$error_test, type = 'o')
plot(out_iter100$error_test, type = 'o')
print(out_iter25$error_test[length(out_iter25$error_test)])
print(out_iter100$error_test[length(out_iter100$error_test)])
# [ToDo] Use microbenchmark to time your code with lambda=1 and 50 iterations. To save time, only apply microbenchmark 5 times.
microbenchmark(
LRMultiClass(X, Y, Xt, Yt, lambda = 1, numIter = 50),
svd(matrix(rnorm(50000), 1000, 500)),
times = 5
)
# [ToDo] Report the median time of your code from microbenchmark above in the comments below
# Median time:  3342.1790 (in milliseconds)
plot(out_lambda05$objective, type = 'o')
# normal cases
source("FunctionsLR.R")
# actual dataset: iris
iris_data <- iris
# convert y to 0~k class
iris_data$Species <- as.numeric(iris_data$Species) - 1
# permute
iris_data = iris_data[sample(nrow(iris_data)), ]
iris_data
# divide train and test
iris_train = iris_data[1:((0.8) * nrow(iris_data)), ]
iris_train
iris_test = iris_data[(0.8 * nrow(iris_data)):nrow(iris_data), ]
iris_test
# y
iris_y = as.vector(iris_train$Species)
iris_yt = as.vector(iris_test$Species)
# X
iris_X = cbind(rep(1, nrow(iris_train)), iris_train[, -ncol(iris_train)])
iris_X = as.matrix(iris_X)
iris_Xt = cbind(rep(1, nrow(iris_test)), iris_test[, -ncol(iris_test)])
iris_Xt = as.matrix(iris_Xt)
# compare performance
library(nnet)
iris_package = cbind(rep(1, nrow(iris_train)), iris_train)
# iris_package
package_model <- multinom(Species ~ ., data = iris_package)
my_model <- LRMultiClass(iris_X, iris_y, iris_Xt, iris_yt, numIter = 100)
# error by package
(sum(max.col(package_model$fitted.values) != (iris_y + 1)) / length(iris_y)) * 100
# error of mine
my_model
my_model$beta
package_model$
microbenchmark(
LRMultiClass(iris_X, iris_y, iris_Xt, iris_yt, numIter = 100),
multinom(Species ~ ., data = iris_package)
)
package_model$weights
summary(package_model)
my_model$beta
iris_y + 1
# This is a script to save your own tests for the function
source("FunctionsLR.R")
# compatibility check
# all 1 col
X = matrix(c(rep(1, 5), rep(2, 5), rep(3, 5)), nrow = 5, ncol = 3)
any(X[ , 1] != 1)
X = matrix(c(rep(2, 5), rep(1, 5), rep(3, 5)), nrow = 5, ncol = 3)
any(X[ , 1] != 1)
# getting prob's from mat p
X = matrix(c(1,3,2,4,8,6,3,10,1,5,6,2), nrow = 4, ncol = 3)
X
y = c(0, 2, 0, 1)
# using techniques to access double array by addresses in C
t(X)[seq(0, nrow(X) - 1) * ncol(X) + (y + 1)]
seq(0, (nrow(X) - 1) * ncol(X), by = ncol(X))
seq(0, nrow(X) - 1) * ncol(X)
library(microbenchmark)
microbenchmark(
seq(0, (nrow(X) - 1) * ncol(X), by = ncol(X)),
seq(0, nrow(X) - 1) * ncol(X)
)
X[(seq(nrow(X)) - 1) * ncol(X) + y]
# creating indicator matrix
# indi_ij == indicator(y_i = j) where i:nsample and j:class
n = 10
k = 3
y = c(0,1,2,1,0,0,1,2,0,1)
y_adjusted = y + 1
indi = matrix(0, nrow = n, ncol = k)
indi[cbind(seq_along(y_adjusted), y_adjusted)] <- 1
length(y_adjusted)
indi
# Can we vectorize hessian? diag %*% mat
# diag = n * n
n = 4
p = 2
k = 3
# p mat is n * k
p_mat = matrix(seq(1,12), nrow = n, ncol = k)
p_mat
# d = seq(n)
# X = n * p
X = matrix(seq(1,8), nrow = n, ncol = p)
X
dim(outer(p_mat, X, "*"))[1,1, , ]
outer(p_mat, X, "*") [2, 1, , ]
mat_to_prod = matrix(1, nrow = 3, ncol = 2)
c(1,3,2) * mat_to_prod
crossprod(mat_to_prod, c(1,3,2) * mat_to_prod)
c(1,3,2) * (1 - c(2,1,3))
p_k_col = seq(n)
p_k_col
(p_k_col * (1-p_k_col)) * X
X
# test solve
solve(diag(seq(n), nrow = n), diag(1, nrow = n))
# tests on random matrix
n = 100
p = 10
k = 20
X_not_1 = matrix(rnorm(n * p), nrow = n, ncol = p)
y = sample(k, n, replace = TRUE) - 1
n_t = 20
X_t_not_1 = matrix(rnorm(n_t * p), nrow = n_t, ncol = p)
y_t = sample(k, n_t, replace = TRUE) - 1
# first row stop condition works?
LRMultiClass(X_not_1, y, X_t_not_1, y_t)
# Incompatible dimension
# generating function
X_generator <- function(n, p) {
X = matrix(rnorm(n * (p - 1)), nrow = n, ncol = (p - 1))
# 1's for first row
X = cbind(rep(1, n), X)
return(X)
}
# sample mismatch
missample_X = X_generator(n, p)
missample_Xt = X_generator(n_t + 1, p)
LRMultiClass(missample_X, y, missample_Xt, y_t)
# covariates mismatch
lackcovariate_X = X_generator(n, p - 1)
Xt = X_generator(n_t, p)
LRMultiClass(lackcovariate_X, y, Xt, y_t)
# normal cases
source("FunctionsLR.R")
X = X_generator(n, p)
Xt = X_generator(n_t, p)
# dim(X)
# dim(Xt)
LRMultiClass(X, y, Xt, y_t)
# debug
indi = matrix(0, nrow = n, ncol = k)
dim(indi)
print(length(y_adjusted))
print(seq_along(y_adjusted), y_adjusted)
indi[cbind(seq_along(y_adjusted), y_adjusted)] <- 1
# actual dataset: iris
iris_data <- iris
# convert y to 0~k class
iris_data$Species <- as.numeric(iris_data$Species) - 1
# permute
iris_data = iris_data[sample(nrow(iris_data)), ]
iris_data
# divide train and test
iris_train = iris_data[1:((0.8) * nrow(iris_data)), ]
iris_train
iris_test = iris_data[(0.8 * nrow(iris_data)):nrow(iris_data), ]
iris_test
# y
iris_y = as.vector(iris_train$Species)
iris_yt = as.vector(iris_test$Species)
# X
iris_X = cbind(rep(1, nrow(iris_train)), iris_train[, -ncol(iris_train)])
iris_X = as.matrix(iris_X)
iris_Xt = cbind(rep(1, nrow(iris_test)), iris_test[, -ncol(iris_test)])
iris_Xt = as.matrix(iris_Xt)
# compare performance
library(nnet)
iris_package = cbind(rep(1, nrow(iris_train)), iris_train)
# iris_package
package_model <- multinom(Species ~ ., data = iris_package)
my_model <- LRMultiClass(iris_X, iris_y, iris_Xt, iris_yt, numIter = 100)
# error by package
(sum(max.col(package_model$fitted.values) != (iris_y + 1)) / length(iris_y)) * 100
# error of mine
my_model
my_model$beta
summary(package_model)
microbenchmark(
LRMultiClass(iris_X, iris_y, iris_Xt, iris_yt, numIter = 100),
multinom(Species ~ ., data = iris_package)
)
# Application of multi-class logistic to letters data
# Load the letter data
#########################
# Training data
letter_train <- read.table("Data/letter-train.txt", header = F, colClasses = "numeric")
Y <- letter_train[, 1]
X <- as.matrix(letter_train[, -1])
# Testing data
letter_test <- read.table("Data/letter-test.txt", header = F, colClasses = "numeric")
Yt <- letter_test[, 1]
Xt <- as.matrix(letter_test[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n = nrow(X)
ntest = nrow(Xt)
X = cbind(rep(1, n), X)
Xt = cbind(rep(1, ntest), Xt)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X, Y, Xt, Yt)
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out$error_train, type = 'o')
plot(out$error_test, type = 'o')
# result of the last step
print(out$objective[length(out$objective)])
print(out$error_train[length(out$error_train)])
print(out$error_test[length(out$error_test)])
# Feel free to modify the code above for different lambda/eta/numIter values to see how it affects the convergence as well as train/test errors
# larger and smaller lambda
out_lambda2 = LRMultiClass(X, Y, Xt, Yt, lambda = 2)
out_lambda05 = LRMultiClass(X, Y, Xt, Yt, lambda = 0.5)
# objective with diff lambda
plot(out_lambda2$objective, type = 'o')
plot(out_lambda05$objective, type = 'o')
# compare last step
print(out_lambda2$objective[length(out_lambda2$objective)])
print(out_lambda05$objective[length(out_lambda05$objective)])
# train error with diff lambda
plot(out_lambda2$error_train, type = 'o')
plot(out_lambda05$error_train, type = 'o')
print(out_lambda2$error_train[length(out_lambda2$error_train)])
print(out_lambda05$error_train[length(out_lambda05$error_train)])
# test error with diff lambda
plot(out_lambda2$error_test, type = 'o')
plot(out_lambda05$error_test, type = 'o')
print(out_lambda2$error_test[length(out_lambda2$error_test)])
print(out_lambda05$error_test[length(out_lambda05$error_test)])
# larger and smaller eta
out_eta05 = LRMultiClass(X, Y, Xt, Yt, eta = 0.5)
out_eta001 = LRMultiClass(X, Y, Xt, Yt, eta = 0.01)
# objective with diff eta
plot(out_eta05$objective, type = 'o')
plot(out_eta001$objective, type = 'o')
print(out_eta05$objective[length(out_eta05$objective)])
print(out_eta001$objective[length(out_eta001$objective)])
# train error with diff eta
plot(out_eta05$error_train, type = 'o')
plot(out_eta001$error_train, type = 'o')
print(out_eta05$error_train[length(out_eta05$error_train)])
print(out_eta001$error_train[length(out_eta001$error_train)])
# test error with diff eta
plot(out_eta05$error_test, type = 'o')
plot(out_eta001$error_test, type = 'o')
print(out_eta05$error_test[length(out_eta05$error_test)])
print(out_eta001$error_test[length(out_eta001$error_test)])
# larger and smaller Numiter
out_iter25 = LRMultiClass(X, Y, Xt, Yt, numIter = 25)
out_iter100 = LRMultiClass(X, Y, Xt, Yt, numIter = 100)
# objective with diff numIter
plot(out_iter25$objective, type = 'o')
plot(out_iter100$objective, type = 'o')
print(out_iter25$objective[length(out_iter25$objective)])
print(out_iter100$objective[length(out_iter100$objective)])
# train error with diff iter
plot(out_iter25$error_train, type = 'o')
plot(out_iter100$error_train, type = 'o')
print(out_iter25$error_train[length(out_iter25$error_train)])
print(out_iter100$error_train[length(out_iter100$error_train)])
# test error with diff iter
plot(out_iter25$error_test, type = 'o')
plot(out_iter100$error_test, type = 'o')
print(out_iter25$error_test[length(out_iter25$error_test)])
print(out_iter100$error_test[length(out_iter100$error_test)])
# [ToDo] Use microbenchmark to time your code with lambda=1 and 50 iterations. To save time, only apply microbenchmark 5 times.
microbenchmark(
LRMultiClass(X, Y, Xt, Yt, lambda = 1, numIter = 50),
svd(matrix(rnorm(50000), 1000, 500)),
times = 5
)
# [ToDo] Report the median time of your code from microbenchmark above in the comments below
# Median time:  3342.1790 (in milliseconds)
# Application of multi-class logistic to letters data
# Load the letter data
#########################
# Training data
letter_train <- read.table("Data/letter-train.txt", header = F, colClasses = "numeric")
Y <- letter_train[, 1]
X <- as.matrix(letter_train[, -1])
# Testing data
letter_test <- read.table("Data/letter-test.txt", header = F, colClasses = "numeric")
Yt <- letter_test[, 1]
Xt <- as.matrix(letter_test[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n = nrow(X)
ntest = nrow(Xt)
X = cbind(rep(1, n), X)
Xt = cbind(rep(1, ntest), Xt)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X, Y, Xt, Yt)
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out$error_train, type = 'o')
plot(out$error_test, type = 'o')
# result of the last step
print(out$objective[length(out$objective)])
print(out$error_train[length(out$error_train)])
print(out$error_test[length(out$error_test)])
# Feel free to modify the code above for different lambda/eta/numIter values to see how it affects the convergence as well as train/test errors
# larger and smaller lambda
out_lambda2 = LRMultiClass(X, Y, Xt, Yt, lambda = 2)
out_lambda05 = LRMultiClass(X, Y, Xt, Yt, lambda = 0.5)
# objective with diff lambda
plot(out_lambda2$objective, type = 'o')
plot(out_lambda05$objective, type = 'o')
# compare last step
print(out_lambda2$objective[length(out_lambda2$objective)])
print(out_lambda05$objective[length(out_lambda05$objective)])
# train error with diff lambda
plot(out_lambda2$error_train, type = 'o')
plot(out_lambda05$error_train, type = 'o')
print(out_lambda2$error_train[length(out_lambda2$error_train)])
print(out_lambda05$error_train[length(out_lambda05$error_train)])
# test error with diff lambda
plot(out_lambda2$error_test, type = 'o')
plot(out_lambda05$error_test, type = 'o')
print(out_lambda2$error_test[length(out_lambda2$error_test)])
print(out_lambda05$error_test[length(out_lambda05$error_test)])
# larger and smaller eta
out_eta05 = LRMultiClass(X, Y, Xt, Yt, eta = 0.5)
out_eta001 = LRMultiClass(X, Y, Xt, Yt, eta = 0.01)
# objective with diff eta
plot(out_eta05$objective, type = 'o')
plot(out_eta001$objective, type = 'o')
print(out_eta05$objective[length(out_eta05$objective)])
print(out_eta001$objective[length(out_eta001$objective)])
# train error with diff eta
plot(out_eta05$error_train, type = 'o')
plot(out_eta001$error_train, type = 'o')
print(out_eta05$error_train[length(out_eta05$error_train)])
print(out_eta001$error_train[length(out_eta001$error_train)])
# test error with diff eta
plot(out_eta05$error_test, type = 'o')
plot(out_eta001$error_test, type = 'o')
print(out_eta05$error_test[length(out_eta05$error_test)])
print(out_eta001$error_test[length(out_eta001$error_test)])
# larger and smaller Numiter
out_iter25 = LRMultiClass(X, Y, Xt, Yt, numIter = 25)
out_iter100 = LRMultiClass(X, Y, Xt, Yt, numIter = 100)
# objective with diff numIter
plot(out_iter25$objective, type = 'o')
plot(out_iter100$objective, type = 'o')
print(out_iter25$objective[length(out_iter25$objective)])
print(out_iter100$objective[length(out_iter100$objective)])
# train error with diff iter
plot(out_iter25$error_train, type = 'o')
plot(out_iter100$error_train, type = 'o')
print(out_iter25$error_train[length(out_iter25$error_train)])
print(out_iter100$error_train[length(out_iter100$error_train)])
# test error with diff iter
plot(out_iter25$error_test, type = 'o')
plot(out_iter100$error_test, type = 'o')
print(out_iter25$error_test[length(out_iter25$error_test)])
print(out_iter100$error_test[length(out_iter100$error_test)])
# [ToDo] Use microbenchmark to time your code with lambda=1 and 50 iterations. To save time, only apply microbenchmark 5 times.
microbenchmark(
LRMultiClass(X, Y, Xt, Yt, lambda = 1, numIter = 50),
svd(matrix(rnorm(50000), 1000, 500)),
times = 5
)
# [ToDo] Report the median time of your code from microbenchmark above in the comments below
# Median time:  3342.1790 (in milliseconds)
# [ToDo] Use microbenchmark to time your code with lambda=1 and 50 iterations. To save time, only apply microbenchmark 5 times.
library(microbenchmark)
microbenchmark(
LRMultiClass(X, Y, Xt, Yt, lambda = 1, numIter = 50),
svd(matrix(rnorm(50000), 1000, 500)),
times = 5
)
microbenchmark(
LRMultiClass(X, Y, Xt, Yt, lambda = 1, numIter = 50),
svd(matrix(rnorm(50000), 1000, 500)),
times = 5
)
