library(microbenchmark)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 100),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 100),
times = 5
)
remove.packages("GroupHW")
# Load the letter data
#########################
# Training data
letter_train <- read.table("letter-train.txt", header = F, colClasses = "numeric")
Y1 <- letter_train[, 1]
X1 <- as.matrix(letter_train[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n1 = nrow(X1)
X1 = cbind(rep(1, n1), X1)
# Source the LR function
source("FunctionsLR.R")
install.packages("HroupHW", dependencies=TRUE, INSTALL_opts = c('--no-lock'))
install.packages("GroupHW", dependencies=TRUE, INSTALL_opts = c('--no-lock'))
install.packages("GroupHW", dependencies = TRUE, INSTALL_opts = c("--no-lock"))
# Load the letter data
#########################
# Training data
letter_train <- read.table("letter-train.txt", header = F, colClasses = "numeric")
Y1 <- letter_train[, 1]
X1 <- as.matrix(letter_train[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n1 = nrow(X1)
library(GroupHW)
X1 = cbind(rep(1, n1), X1)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X1, Y1)
out_c = GroupHW::LRMultiClass(X1, Y1)
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X1, Y1)
out_c = GroupHW::LRMultiClass(X1, Y1)
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
remove.packages("GroupHW")
remove.packages("GroupHW")
library(GroupHW)
# Load the letter data
#########################
# Training data
letter_train <- read.table("letter-train.txt", header = F, colClasses = "numeric")
Y1 <- letter_train[, 1]
X1 <- as.matrix(letter_train[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n1 = nrow(X1)
X1 = cbind(rep(1, n1), X1)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X1, Y1)
out_c = GroupHW::LRMultiClass(X1, Y1)
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out_c$objective, type = 'o')
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 5
)
# [ToDo] Use microbenchmark to time your code with lambda=1 and 50 iterations. To save time, only apply microbenchmark 5 times.
library(microbenchmark)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 5
)
# Application of K-means algorithm to ZIPCODE data
source("FunctionsKmeans.R")
# Rand Index Calculation example
require(fossil)
cluster1 <- c(2,2,1,3)
cluster2 <- c(3,3,2,1)
rand.index(cluster1, cluster2) # clusters match
# Application of K-means algorithm to ZIPCODE data
source("FunctionsKmeans.R")
# Load the ZIPCODE data
zipcode <- read.table("ZIPCODE.txt", header = F)
# Extract the true digits
Y1 <- zipcode[ , 1]
# Extract the data points
X1 <- zipcode[ , -1]
n1 = nrow(X1)
p1 = ncol(X1)
K1 = 10
set.seed(1)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
# [ToDo] Try K-means algorithm nRep times with different starting points on ZIPCODE data. Calculate Rand Index at each replication
nRep <- 5
rand_index = 0
cnt = 0
# iterate for 50 times
for(i in 1:nRep) {
trial <- try(y_hat <<- MyKmeans(X1, K1))
# skip the failed one
if(inherits(trial, "try-error")) {
next
}
cnt = cnt + 1
rand_index = rand_index + rand.index(Y1, y_hat)
}
rand_index = rand_index / cnt
rand_index
cnt
# library(GroupHW)
rand_index_c = 0
cnt_c = 0
for(i in 1:nRep) {
trial <- try(y_hat_c <<- GroupHW::MyKmeans(X1, K1))
# skip the failed one
if(inherits(trial, "try-error")) {
next
}
cnt_c = cnt_c + 1
rand_index_c = rand_index_c + rand.index(Y1, y_hat_c)
}
# [ToDo] Report mean Rand Index
rand_index_c = rand_index_c / cnt_c
rand_index_c
cnt_c
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
print(out$objective[length(out$objective)])
print(out_c$objective[length(out$objective)])
# Feel free to modify the code above for different lambda/eta/numIter values to see how it affects the convergence as well as train/test errors
# larger and smaller lambda
out_lambda2 = LRMultiClass(X1, Y1, lambda = 2)
out_lambda05 = LRMultiClass(X1, Y1, lambda = 0.5)
out_c_lambda2 = GroupHW::LRMultiClass(X1, Y1, lambda = 2)
# Load the letter data
#########################
# Training data
letter_train <- read.table("letter-train.txt", header = F, colClasses = "numeric")
Y1 <- letter_train[, 1]
X1 <- as.matrix(letter_train[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n1 = nrow(X1)
X1 = cbind(rep(1, n1), X1)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X1, Y1)
out_c = GroupHW::LRMultiClass(X1, Y1)
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out_c$objective, type = 'o')
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
print(out$objective[length(out$objective)])
print(out_c$objective[length(out$objective)])
# Feel free to modify the code above for different lambda/eta/numIter values to see how it affects the convergence as well as train/test errors
# larger and smaller lambda
out_lambda2 = LRMultiClass(X1, Y1, lambda = 2)
out_lambda05 = LRMultiClass(X1, Y1, lambda = 0.5)
out_c_lambda2 = GroupHW::LRMultiClass(X1, Y1, lambda = 2)
out_c_lambda05 = GroupHW::LRMultiClass(X1, Y1, lambda = 0.5)
# objective with diff lambda
plot(out_lambda2$objective, type = 'o')
plot(out_c_lambda2$objective, type = 'o')
plot(out_lambda05$objective, type = 'o')
plot(out_c_lambda05$objective, type = 'o')
# compare last step
all.equal(out_lambda2$objective, as.numeric(out_c_lambda2$objective))
all.equal(out_lambda2$beta, out_c_lambda2$beta)
all.equal(out_lambda05$objective, as.numeric(out_c_lambda05$objective))
all.equal(out_lambda05$beta, out_c_lambda05$beta)
# larger and smaller eta
out_eta05 = LRMultiClass(X1, Y1, eta = 0.5)
out_c_eta05 = GroupHW::LRMultiClass(X1, Y1, eta = 0.5)
out_eta001 = LRMultiClass(X1, Y1, eta = 0.01)
out_c_eta001 = GroupHW::LRMultiClass(X1, Y1, eta = 0.01)
all.equal(out_eta05$objective, as.numeric(out_c_eta05$objective))
all.equal(out_eta001$objective, as.numeric(out_c_eta001$objective))
all.equal(out_eta05$beta, out_c_eta05$beta)
all.equal(out_eta001$beta, out_c_eta001$beta)
# larger and smaller Numiter
out_iter25 = LRMultiClass(X1, Y1, numIter = 25)
out_iter100 = LRMultiClass(X1, Y1, numIter = 100)
out_iter25_c = GroupHW::LRMultiClass(X1, Y1, numIter = 25)
out_iter100_c = GroupHW::LRMultiClass(X1, Y1, numIter = 100)
all.equal(out_iter25$beta, out_iter25_c$beta)
all.equal(out_iter25$objective, as.numeric(out_iter25_c$objective))
all.equal(out_iter100$objective, as.numeric(out_iter100_c$objective))
all.equal(out_iter100$beta, out_iter100_c$beta)
# [ToDo] Use microbenchmark to time your code with lambda=1 and 50 iterations. To save time, only apply microbenchmark 5 times.
library(microbenchmark)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 5
)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 10
)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 20
)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
svd(matrix(rnorm(50000), 1000, 500)),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 20
)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 100),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 100),
times = 10
)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 25),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 25),
times = 10
)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 200),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 200),
times = 5
)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 10
)
remove.packages("GroupHW")
library(GroupHW)
# Load the letter data
#########################
# Training data
letter_train <- read.table("letter-train.txt", header = F, colClasses = "numeric")
Y1 <- letter_train[, 1]
X1 <- as.matrix(letter_train[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n1 = nrow(X1)
X1 = cbind(rep(1, n1), X1)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X1, Y1)
out_c = GroupHW::LRMultiClass(X1, Y1)
function (target, current, ...)
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
remove.packages("GroupHW")
# Application of K-means algorithm to ZIPCODE data
source("FunctionsKmeans.R")
# Rand Index Calculation example
require(fossil)
cluster1 <- c(2,2,1,3)
cluster2 <- c(3,3,2,1)
rand.index(cluster1, cluster2) # clusters match
# Load the ZIPCODE data
zipcode <- read.table("ZIPCODE.txt", header = F)
# Extract the true digits
Y1 <- zipcode[ , 1]
# Extract the data points
X1 <- zipcode[ , -1]
n1 = nrow(X1)
p1 = ncol(X1)
K1 = 10
set.seed(1)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
# [ToDo] Try K-means algorithm nRep times with different starting points on ZIPCODE data. Calculate Rand Index at each replication
nRep <- 50
# [ToDo] Try K-means algorithm nRep times with different starting points on ZIPCODE data. Calculate Rand Index at each replication
nRep <- 5
rand_index = 0
cnt = 0
# iterate for 50 times
for(i in 1:nRep) {
trial <- try(y_hat <<- MyKmeans(X1, K1))
# skip the failed one
if(inherits(trial, "try-error")) {
next
}
cnt = cnt + 1
rand_index = rand_index + rand.index(Y1, y_hat)
}
rand_index = rand_index / cnt
rand_index
cnt
# library(GroupHW)
rand_index_c = 0
cnt_c = 0
for(i in 1:nRep) {
trial <- try(y_hat_c <<- GroupHW::MyKmeans(X1, K1))
# skip the failed one
if(inherits(trial, "try-error")) {
next
}
cnt_c = cnt_c + 1
rand_index_c = rand_index_c + rand.index(Y1, y_hat_c)
}
# [ToDo] Report mean Rand Index
rand_index_c = rand_index_c / cnt_c
rand_index_c
cnt_c
# Load the letter data
#########################
# Training data
letter_train <- read.table("letter-train.txt", header = F, colClasses = "numeric")
Y1 <- letter_train[, 1]
X1 <- as.matrix(letter_train[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n1 = nrow(X1)
X1 = cbind(rep(1, n1), X1)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X1, Y1)
out_c = GroupHW::LRMultiClass(X1, Y1)
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out_c$objective, type = 'o')
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out_c$objective, type = 'o')
remove.packages("GroupHW")
library(GroupHW)
# Load the letter data
#########################
# Training data
letter_train <- read.table("letter-train.txt", header = F, colClasses = "numeric")
Y1 <- letter_train[, 1]
X1 <- as.matrix(letter_train[, -1])
# [ToDo] Make sure to add column for an intercept to X and Xt
n1 = nrow(X1)
X1 = cbind(rep(1, n1), X1)
# Source the LR function
source("FunctionsLR.R")
# [ToDo] Try the algorithm LRMultiClass with lambda = 1 and 50 iterations. Call the resulting object out, i.e. out <- LRMultiClass(...)
out = LRMultiClass(X1, Y1)
out_c = GroupHW::LRMultiClass(X1, Y1)
# The code below will draw pictures of objective function, as well as train/test error over the iterations
plot(out$objective, type = 'o')
plot(out_c$objective, type = 'o')
# result of the last step
all.equal(out$objective, as.numeric(out_c$objective))
all.equal(out$beta, out_c$beta)
print(out$objective[length(out$objective)])
print(out_c$objective[length(out$objective)])
# Feel free to modify the code above for different lambda/eta/numIter values to see how it affects the convergence as well as train/test errors
# larger and smaller lambda
out_lambda2 = LRMultiClass(X1, Y1, lambda = 2)
out_lambda05 = LRMultiClass(X1, Y1, lambda = 0.5)
out_c_lambda2 = GroupHW::LRMultiClass(X1, Y1, lambda = 2)
out_c_lambda05 = GroupHW::LRMultiClass(X1, Y1, lambda = 0.5)
# objective with diff lambda
plot(out_lambda2$objective, type = 'o')
plot(out_c_lambda2$objective, type = 'o')
plot(out_lambda05$objective, type = 'o')
plot(out_c_lambda05$objective, type = 'o')
# compare last step
all.equal(out_lambda2$objective, as.numeric(out_c_lambda2$objective))
all.equal(out_lambda2$beta, out_c_lambda2$beta)
all.equal(out_lambda05$objective, as.numeric(out_c_lambda05$objective))
all.equal(out_lambda05$beta, out_c_lambda05$beta)
out_eta05 = LRMultiClass(X1, Y1, eta = 0.5)
out_c_eta05 = GroupHW::LRMultiClass(X1, Y1, eta = 0.5)
out_eta001 = LRMultiClass(X1, Y1, eta = 0.01)
out_c_eta001 = GroupHW::LRMultiClass(X1, Y1, eta = 0.01)
all.equal(out_eta05$objective, as.numeric(out_c_eta05$objective))
all.equal(out_eta001$objective, as.numeric(out_c_eta001$objective))
all.equal(out_eta05$beta, out_c_eta05$beta)
all.equal(out_eta001$beta, out_c_eta001$beta)
out_iter25 = LRMultiClass(X1, Y1, numIter = 25)
out_iter100 = LRMultiClass(X1, Y1, numIter = 100)
out_iter25_c = GroupHW::LRMultiClass(X1, Y1, numIter = 25)
out_iter100_c = GroupHW::LRMultiClass(X1, Y1, numIter = 100)
all.equal(out_iter25$beta, out_iter25_c$beta)
all.equal(out_iter25$objective, as.numeric(out_iter25_c$objective))
all.equal(out_iter100$objective, as.numeric(out_iter100_c$objective))
all.equal(out_iter100$beta, out_iter100_c$beta)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 10
)
# [ToDo] Use microbenchmark to time your code with lambda=1 and 50 iterations. To save time, only apply microbenchmark 5 times.
library(microbenchmark)
microbenchmark(
LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
GroupHW::LRMultiClass(X1, Y1, lambda = 1, numIter = 50),
times = 10
)
# Application of K-means algorithm to ZIPCODE data
source("FunctionsKmeans.R")
# Rand Index Calculation example
require(fossil)
cluster1 <- c(2,2,1,3)
cluster2 <- c(3,3,2,1)
rand.index(cluster1, cluster2) # clusters match
# Load the ZIPCODE data
zipcode <- read.table("ZIPCODE.txt", header = F)
# Extract the true digits
Y1 <- zipcode[ , 1]
# Extract the data points
X1 <- zipcode[ , -1]
n1 = nrow(X1)
p1 = ncol(X1)
K1 = 10
set.seed(1)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
# [ToDo] Try K-means algorithm nRep times with different starting points on ZIPCODE data. Calculate Rand Index at each replication
nRep <- 5
# [ToDo] Try K-means algorithm nRep times with different starting points on ZIPCODE data. Calculate Rand Index at each replication
nRep <- 50
rand_index = 0
cnt = 0
# iterate for 50 times
for(i in 1:nRep) {
trial <- try(y_hat <<- MyKmeans(X1, K1))
# skip the failed one
if(inherits(trial, "try-error")) {
next
}
cnt = cnt + 1
rand_index = rand_index + rand.index(Y1, y_hat)
}
rand_index = rand_index / cnt
rand_index
cnt
# library(GroupHW)
rand_index_c = 0
cnt_c = 0
for(i in 1:nRep) {
trial <- try(y_hat_c <<- GroupHW::MyKmeans(X1, K1))
# skip the failed one
if(inherits(trial, "try-error")) {
next
}
cnt_c = cnt_c + 1
rand_index_c = rand_index_c + rand.index(Y1, y_hat_c)
}
# [ToDo] Report mean Rand Index
rand_index_c = rand_index_c / cnt_c
rand_index_c
cnt_c
set.seed(1)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
# [ToDo] Try K-means algorithm nRep times with different starting points on ZIPCODE data. Calculate Rand Index at each replication
nRep <- 50
res_base
res_cur
rand.index(res_base, Y1)
rand.index(res_cur, Y1)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
set.seed(1)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
rand.index(res_base, Y1)
rand.index(res_cur, Y1)
res_base
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
table(res_base)
table(res_cur)
rand.index(res_base, Y1)
rand.index(res_cur, Y1)
res_base = MyKmeans(X1, K1)
res_cur = GroupHW::MyKmeans(X1, K1)
rand.index(res_base, Y1)
rand.index(res_cur, Y1)
